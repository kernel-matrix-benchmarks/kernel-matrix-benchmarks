# This file is processed in kernel_matrix_benchmarks/algorithms/definitions.py
# by the routine "get_definitions(...)"

# Full syntax example:
nicealgorithm:
  disabled: true # This example is just here to showcase the syntax.

  # Specify the hardware on which this algorithm should be run:
  # - CPU for a "r5b.16xlarge" instance on AWS EC2:
  #   - Spot price ~ $0.70/h.
  #   - 64vCPU - 2nd gen Intel Xeon Scalable (Cascade Lake) with AVX-512 support.
  #   - No GPU.
  #   - 512Gb RAM.
  # - GPU for a "p3.2xlarge" instance on AWS EC2:
  #   - Spot price ~ $0.92/h.
  #   - 8vCPU - Intel Xeon E5-2686 v4 (Broadwell) with AVX2 but not AVX-512.
  #   - 1 Tesla V100 GPU with 16Gb "RAM", 5,120 CUDA cores and 620 Tensor cores.
  #   - 61Gb RAM.
  hardware: CPU

  # Three boolean flags (all equal to false by default)
  product: true # This model can be used for kernel matrix products...
  attention: false # But not for attention layers (= row-normalize kernel products)
  solver: false # or kernel matrix solvers.

  # Where can we find the Python API for this method?
  docker-tag: kernel-matrix-benchmarks-nicedocker # built from install/Dockerfile.nicedocker
  module: kernel_matrix_benchmarks.algorithms.nicelibrary # Python module
  constructor: NiceAlgorithm # Python class name

  # List of parameter values.
  # Every algorithm can be run with different parameter values,
  # specified using a collection of run-groups. Each run-group is defined using:
  #
  # - datasets: a list of string expressions.
  #     If any of these expressions matches the name of the dataset/experiment,
  #     the run-group will be used. Note that all datasets are named as:
  #     "{task}-{dataset-label}-D{Dimpoints}-E{Dimsignal}-M{Nsource}-N{Ntarget}-{kernel-name}".
  #     Consequently, using e.g. "*-gaussian" is a simple way of filtering
  #     experiments that are designed for Gaussian kernels.
  #
  # - args: a list of dict of "**kwargs".
  #     Each run starts by instantiating the class constructor (specified above)
  #     with these arguments and two extra keyword arguments:
  #       - kernel: str, the name of the kernel function.
  #       - dimension: int, the dimension D of the source and target points.
  #
  # - query-args: a list of dict of "**kwargs".
  #     After the pre-processing phase (= construction of the kernel matrix)
  #     but before the actual query, we may want to test different values
  #     for parameters "at query time" without having to go through
  #     the pre-processing phase every time.
  #     This is enabled by the query-args parameters, which are fed to:
  #       algo.set_query_arguments(**query-args)
  #     after the pre-computation phase, but before the query.
  #
  # N.B.: We enforce a soft limit of up to 10 different runs per method per dataset.
  #       The goal here is to keep things simple for library developers
  #       (so that they do not have to tune a couple of "optimal" parameter values
  #       per dataset) while keeping the cost of running the full benchmarks
  #       on the AWS EC2 cloud reasonable.
  run-groups:
    simple-run: # Choose an arbitrary name for the group of runs.
      datasets: # Match all experiments with...
        - "*-E1-*-gaussian" # Gaussian kernels and scalar signal *or* ...
        - "*-D3-*" # source and target points in dimension 3.
      args: # Appended to {"kernel": ..., "dimension": ...} when calling the constructor.
        - { "K": 10, "tol": 0.01 }

    two-runs:
      datasets:
        - "*-sphere-*-inverse-distance"
      args:
        - { "K": 10, "tol": 0.01 }
        - { "K": 50, "tol": 0.001 }

    with-query-args:
      datasets:
        - "*" # Match all experiments!
      args:
        - { "K": 2, "tol": 0.1 }
        - { "K": 5, "tol": 0.1 }
      query-args:
        - { "h": 3 }
        - { "h": 9 }

  # All in all, the code above defines:
  # 1 run for datasets generated with a Gaussian or an absolute exponential kernel:
  # - Instantiation with:
  #   algo = NiceAlgorithm(kernel=..., dimension=D, K=10, tol=0.01)
  #   then pre-computations with: algo.prepare_data(...) and algo.fit()
  #   then no query arguments with: algo.set_query_arguments()
  #   then query with: algo.prepare_query(...), algo.query(), algo.get_result()
  #
  # *and* 2 runs for datasets sampled on the sphere with an inverse distance kernel:
  # - Instantiation with:
  #   algo = NiceAlgorithm(kernel=..., dimension=D, K=10, tol=0.01)
  #   and no query arguments.
  # - Instantiation with:
  #   algo = NiceAlgorithm(kernel=..., dimension=D, K=50, tol=0.001)
  #   and no query arguments.
  #
  # *and* 4 additional runs for all datasets:
  # - Instantiation with:
  #   algo = NiceAlgorithm(kernel=..., dimension=D, K=2, tol=0.1)
  #   and 2 runs with different query arguments:
  #   - algo.set_query_arguments(h=3)
  #   - algo.set_query_arguments(h=9)
  # - Instantiation with:
  #   algo = NiceAlgorithm(kernel=..., dimension=D, K=5, tol=0.1)
  #   and 2 runs with different query arguments:
  #   - algo.set_query_arguments(h=3)
  #   - algo.set_query_arguments(h=9)
  #
  # Please note that in order to keep run times and costs manageable,
  # we allow a maximum of **10 runs per method per dataset**
  # so don't be too greedy ;-)

dummy-product:
  disabled: false
  hardware: CPU
  product: true
  docker-tag: kernel-matrix-benchmarks-sklearn
  module: kernel_matrix_benchmarks.algorithms.dummy_algo
  constructor: DummyProduct

  run-groups:
    dummy-run:
      datasets:
        - "*" # Match all experiments

dummy-solver:
  disabled: false
  hardware: CPU
  solver: true
  docker-tag: kernel-matrix-benchmarks-sklearn
  module: kernel_matrix_benchmarks.algorithms.dummy_algo
  constructor: DummySolver

  run-groups:
    dummy-run:
      datasets:
        - "*" # Match all experiments

bruteforce-product-blas:
  disabled: false
  hardware: CPU
  product: true
  docker-tag: kernel-matrix-benchmarks-sklearn
  module: kernel_matrix_benchmarks.algorithms.bruteforce
  constructor: BruteForceProductBLAS
  run-groups:
    numerical-precision:
      datasets:
        - "*" # Match all experiments
      args:
        - { "precision": "float16" }
        - { "precision": "float32" }
        - { "precision": "float64" }

bruteforce-solver-blas:
  disabled: false
  hardware: CPU
  solver: true
  docker-tag: kernel-matrix-benchmarks-sklearn
  module: kernel_matrix_benchmarks.algorithms.bruteforce
  constructor: BruteForceSolverLAPACK
  run-groups:
    numerical-precision:
      datasets:
        - "*" # Match all experiments
      args:
        - { "precision": "float16" }
        - { "precision": "float32" }
        - { "precision": "float64" }

htool-product:
  disabled: false
  hardware: CPU
  product: true
  attention: true
  docker-tag: kernel-matrix-benchmarks-htool
  module: kernel_matrix_benchmarks.algorithms.htool
  constructor: HtoolProduct
  run-groups:
    numerical-precision:
      datasets:
        - "*" # Match all experiments
      args:
        - {
            "eta": 100,
            "epsilon": 0.001,
            "source_minclustersize": 100,
            "symmetry": "S",
            "UPLO": "L",
          }
